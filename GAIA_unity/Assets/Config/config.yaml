behaviors:
    PlayerAgent:
        trainer_type: ppo
        max_steps: 10e6                          # Total training steps
        time_horizon: 512                       # Improves long-term reward stability
        summary_freq: 5000                      # Less frequent summaries for larger batches

        hyperparameters:
            batch_size: 256                       # Higher batch improves training signal
            buffer_size: 20480                    # Matches batch size Ã— 80
            learning_rate: 0.0002                 # Slightly lower for stability
            learning_rate_schedule: constant
            beta: 0.005                           # Encourages moderate exploration
            beta_schedule: constant
            epsilon: 0.2                          # PPO clip range
            epsilon_schedule: constant
            lambd: 0.95                           # Better long-term smoothing for reward estimation
            num_epoch: 4                          # One more epoch for stronger convergence

        network_settings:
            normalize: true
            hidden_units: 256
            num_layers: 2

        reward_signals:
            extrinsic:
                gamma: 0.995                        # Slightly higher discount for exploration-based rewards
                strength: 1.0
